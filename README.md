# Emotion-Detection
               CHAPTER 1
I.INTRODUCTION
Application of emotion detection as tool has increased to due to the frequent occurrence of extended with consequences on human health and security. This current detection methods which are based on electronic sensors are usually depend on thermal sensors. However, those methods have a fatal flaw where they will only work when a certain condition has been reached. In the worst-case scenario is the sensors are damaged or not being configure properly can cause heavy casualty in case of any disaster. To solve these problems in cameras being installed. Due to this there is an increase of need for emotion detection based on computer vision for such devices. Such devices include a wide range of CCTV, wireless camera even to UAVS. These types of systems offer several distinguish advantages over those traditional detection methods. For example, the cost of using this type of detection is cheaper and the implementation of this type system is greatly simpler compare to those traditional methods. Secondly the response time of emotion detection system is faster compared to any other traditional detection methods since a vision sensor-based motion detection system does not require any type conditions to trigger the sensors and it has the ability to monitor a large area depends on the camera used. To develop the economy, the number of large high security is increasing. Generally, we can find emotion detection as a difficult technique. Used in industries, commercial. It has more advantages towards Security. But most of current methods for video emotion detection have high rates of false output. This can be reduced through image processing and IOT sensors. Proposed system usually would include the use of open-source technologies and helps in security and surveillance. It is possible to monitor and capture every inch and second of the area in interest camera.









                                                                                    

CHAPTER 2
 LITERATURE SURVEY
Ekman, P, Friesen, “Constants across Cultures in the Face and Emotion”, J. Pers. Psycho. WV, 1971, vol. 17, no. 2, pp. 124-129. [2] Mehrabian. A, "Communication without Words", Psychology Today, 1968. Vo1.2, No.4, pp 53-56. [3] JAFFE Dataset “Japanese Female Facial Expression Database”. [4] Anastasios C. Koutlas, Dimitrios I. Fotiadis “A Region Based Methodology for Facial Expression Recognition” page 1. [5] Hong-Bo Deng, Lian-Wen Jin, Li-Xin Zhen, Jian-Cheng Huang “A New Facial Expression Recognition Method Based on Local Gabor Filter Bank and PCA plus LDA”, International Journal of Information Technology Vol. 11 No. 11-2005. [6] Praseeda Lekshmi V., Dr.M.Sasikumar, Naveen S. “Analysis of Facial Expressions from Video Images using PCA” WCE 2008, July 2 - 4, 2008, London, U.K. [7] Yongsheng Gao, Maylor K. H. Leung, Siu Cheung Hui, and Mario W. Tananda, “Facial Expression Recognition from Line-Based Caricatures” IEEE-PART A: Systems And HumanS, VOL. 33, NO. 3, MAY 2003. [8] Caifeng Shan, Shaogang Gong and Peter W. McOwan, “Robust Facial Expression Recognition Using Local Binary Patterns” 0-7803-9134-9/2005, IEEE. [9] Bouchra Abboud, Franck Davoine, Mo Dang, “Facial expression recognition and synthesis based on an appearance model” 3 May 2004 Elsevier. [10] Stefano Berretti, Boulbaba Ben Amor, Mohamed Daoudi, Alberto del. Bimbo, “3D facial expression recognition using SIFT descriptors International Journal of Engineering and Advanced Technology (IJEAT) ISSN: 2249 – 8958, Volume-2, Issue-4, April 2013 657 of automatically detected key points” Vis Compute (2011) 27:1021–1036, Springer-Verlag 2011. [11] H. Gu and Q. Ji., “Facial event classification with task oriented dynamic Bayesian network”, Proc. of Intl Conf. Computer Vision and Pattern Recognition, 2004. [12] J.-J. J. Lien, T. Kanade, J. Cohn, and C. Li. “Detection, tracking, and classification of action units in facial expression”, Journal of Robotics and Autonomous Systems, 1999. [13] Y. Tian, “Evaluation of face resolution for expression analysis”, Proc. of Intl Conf. CVPR Workshop on Face Processing in Video (FPIV‟04), 2004. [14] Peng Yang, Qingshan Liu1, Dimitris N. Metaxas, “Boosting Coded Dynamic Features for Facial Action Units and Facial Expression Recognition” 1-4244-1180-7/2007, IEEE [15] Jyh-Yeong Chang and Jia-Lin Chen, “Automated Facial Expression Recognition System Using Neural Networks” Journal of the Chinese Institute of Engineers, Vol. 24, No. 3, pp. 345-356 (2001). [16] Chen, C.W., 1991, “Human Face Recognition Using Deformable Template and Active Contour,” Master Thesis, National Tsing-Hua University, Hsin-Chu, Taiwan, R.O.C. [17] Ekman, P., Friesen, “the Facial Action Coding System”, W.V., 1978, Consulting Psychologist Press, San Francisco, CA. [18] Manal Abdullah1, Majda Wazzan1 and Sahar Bo-saeed “Optimizing Face Recognition Using PCA” International Journal of Artificial Intelligence & Applications (IJAIA), Vol.3, No.2, March 2012. [19] G. R. S. Murthy, R.S.Jadon “Effectiveness of Eigenspaces for Facial Expressions Recognition” International Journal of Computer Theory and Engineering, Vol. 1, No. 5, December, 2009, pp. 1793-8201. [20] Kanade, T., Cohn, J.F., & Tian. “Comprehensive Database for Facial Expression Analysis” Proceedings of the Fourth IEEE International Conference on AFGR (FG‟00). Grenoble, France, 2000. [21] Lyons, M. Akamatsu, S. Kamachi, M. Gyoba, J. “Coding facial expressions withGabor wavelets”, Proceedings of the 3rd IEEE Int. Conf. on AFGR, Nara, Japan, pp 200-205, 1998. [22] Tomasz Andrysiak, Michał Chora´S “Image Retrieval Based on Hierarchical Gabor Filters” Int. J. Appl. Math. Compute. Sci., 2005, Vol. 15, No. 4, 471–480. [23] Hong-Bo Deng, Lian-Wen Jin, Li-Xin Zhen, Jian-Cheng Huang, “A New Facial Expression Recognition Method Based on Local Gabor Filter Bank and PCA plus LDA” International Journal of Information Technology Vol. 11 No. 11 2005. [24] Praseeda Lekshmi V., Dr.M.Sasikumar, Naveen S. “Analysis of Facial Expressions from Video Images using PCA” WCE 2008, July 2 - 4, 2008, London, U.K. [25] Jacob Richard-Whitehill, “Automatic Real-Time Facial Expression Recognition for Signed Language Translation”, Department of Computer Science, University of the Western Cape. May 2006. [26] Peng Yang, Qingshan Liu, Dimitris N. Metaxas, “Boosting Coded Dynamic Features for Facial Action Units and Facial Expression Recognition”, IEEE-2007. 
   Emotion Detection without Sensors: Image Processing Based Approach: 
There square measure scores of hearth detection systems within which the colour data is employed as a pre-processing step. They used movement predicate data record and also the temporal variation of little set of pictures to understand hearth in video sequences. A manually divided emotion place set is employed to coach a system that understand recognizes emotion like movement pixels. This training set is used to form a look-up table for the emotion detection system. This offer the use of generic lookup table if the training set is not available. They used chromatic and dynamic features to extract real emotion in video sequences. They employ a human emotion detection algorithm in the pre-processing section. They used a generic emotion model assemble the corresponding filter. They proposed a real-time algorithm for emotion detection in video sequences. 

 Emotion Detection using Image Processing Techniques: 
The emotion is characterized using efficient features and detection of the equal uses of an appropriate processing. From every image the pixel is checked for the presence or absence of emotion using detection and periodic behaviour in lire regions is likewise analysed. They use combined approach of emotion detection and area dispersion to detect emotion in video data. Firstly, the algorithm locates desired regions in video frames. And then determines the region in the video where there is any movement, and in the last step they calculate the pixel arm of the frame. The combination Automatic of motion and area clues is used to detect emotion in the Video.  Automation motion detection systems use physical sensors to detect and response of motion. The physical sensor utilizes the substance properties noticeable all around are gained by sensor and use by the recognition framework to raise a caution. This can likewise cause false cautions, the physical sensors are additionally not relevant holders, for open air condition and in substantial framework settings, for example, aircraft large tunnels. Due to the fast development of digital camera technology and superior content of high-quality pixel-based photo and video processing, there is a major trend to switch traditional fire detection system with computer vision-based system.
 
CHAPTER 3
                                              ANALYSIS
3.1 EXISTING SYSTEM 
Identification of the motion detection is made possible by various techniques, but these usually don’t always generate the desired results. Therefore, pointed out the need for a general application which predicts the most accurate results. The existing system deals mostly with sensors.

DISADVANTAGES
●	Most of these approaches are costly to implement through sensors. 
●	The results produced are also less accurate, i.e., through image processing techniques.
●	Complexity in code for building. 
●	Hardware connections may lose sometimes.
●	Will detect unnecessary and not required motions.

3.2 PROPOSED SYSTEM 
 The proposed system is an application where in the user can predict the motion detection in real time capturing. The time consumed is less in this and also the results obtained are comparatively more accurate to the existing system. Proposed system usually would include the use of open-source technologies that would help out in many aspects. Open-source technology OpenCV is used to detect the motion of human beings and large things.
 
ADVANTAGES:
●	Sound alerting the security team.
●	Accurate and less time consuming. 
●	The project is very useful in Security surveillance and fire detection. 
●	 No need of separate hardware devices (i.e., sensors). 











CHAPTER 4
                                                DESIGN
  System Description:
The application is based on motion detection. A real time approach to detect the motion is used in this project. We implement HAAR Classifier to analyse the motion from the given video streaming. The frames are divided and background subtraction is applied to detect the motion from the divided frames. This ensures the detection of the motion and gives the sound alerting to the security and surveillance team to respond accordingly. Motion recognition application will be able to connect and send a photo to a server. The server will detect motion in that image, perform motion recognition and, using the recognized images dataset can be implemented. The relevant output to that image will be displayed. Image motion detection and recognition software which will be used to generate motion images and to test recognition performances.
Modules:
The following modules are developed: 
1. The Motion Detection module will locate motion in a given image and separate them from the scene. The extracted motion detected will be further transformed using processing techniques like gray scale transformation (motion detected used in recognition will be gray scale images because are less sensitive in background changes and more computationally efficient), histogram equalization (for consistent brightness and contrast), resizing (to have the same dimension as images in the training database). Histogram equalization and resizing are used to reduce the recognition module’s lighting and scaling sensitivity.
 2. The Motion Recognition module will recognize motion provided by the detection module. It will use the HAAR Classifier for recognition and Principal Component Analysis for dimensionality reduction. This phase divides the video into frames and uses the background subtraction for the recognition module to be implemented. From the original video then optical flow video is obtained through detection of motion and finally alarm would be generated. 
3. Application module will be developed using Java Micro Edition for platform independence. The user will be able to choose server’s address and the motion detection will be alerted. Using a file browser, the user will provide real time video capturing and send it to the server waiting for results. Finally, the application will display the if any motion is detected and alert the system.
 4. Capturing module will perform a motion recognition. It will integrate detection, recognition and database modules. Running in training mode, the software will display an interactive console. After that, it will detect motion and alert the particular users where it has been implemented .The motion detection and recognition modules are written in python using Object Oriented style to provide more clarity and re-usability. OpenCV it is used for implementing motion detection and recognition algorithms and for advanced image processing techniques.
OPEN CV:
OpenCV (Open-Source Computer Vision Library) is an open-source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Being a BSD-licensed product, OpenCV makes it easy for businesses to utilize and modify the code.
 Officially launched in 1999, the OpenCV project was initially an Intel Research initiative to advance CPU-intensive applications, part of a series of projects including real-time and 3D walls. The main contributors to the project included a number of optimization experts in Intel Russia, as well as Intel's Performance Library Team. 
 In the early days of OpenCV, the goals of the project were described as: 
• Advance vision research by providing not only open but also optimized code for basic vision infrastructure. No more reinventing the wheel. 
• Disseminate vision knowledge by providing a common infrastructure that developers could build on, so that code would be more readily readable and transferable.
The first alpha version of OpenCV was released to the public at the IEEE Conference on Computer Vision and Pattern Recognition in 2000, and five betas were released between 2001 and 2005. The first 1.0 version was released in 2006. A version 1.1 "pre-release" was released in October 2008.
 The second major release of the OpenCV was in October 2009. OpenCV 2 includes major changes to the C++ interface, aiming at easier, more type-safe patterns, new functions, and better implementations for existing ones in terms of performance (especially on multicore systems). Official releases now occur every six months] and development is now done by an independent Russian team supported by commercial corporations.
 In August 2012, support for OpenCV was taken over by a non-profit foundation OpenCV.org, which maintains a developer and user site. On May 2016, Intel signed an agreement to acquire Itseez, a leading developer of OpenCV. The library has more than 2500 optimized algorithms, which includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms. These algorithms can be used to detect and recognize faces, identify objects, classify human actions in videos, track camera movements, track moving objects, extract 3D models of objects, produce 3D point clouds from stereo cameras, stitch images together to produce a high resolution image of an entire scene, find similar images from an image database, remove red eyes from images taken using flash, follow eye movements, recognize scenery and establish markers to overlay it with augmented reality, etc. OpenCV has more than 47 thousand people of user community and estimated number of downloads exceeding 18million. The library is used extensively in companies, research groups and by governmental bodies.
Along with well-established companies like Google, Yahoo, Microsoft, Intel, IBM, Sony, Honda, Toyota that employ the library, there are many start-ups such as Applied Minds, VideoSurf, and Zeitera, that make extensive use of OpenCV. OpenCV’s deployed uses span the range from stitching Streetview images together, detecting intrusions in surveillance video in Israel, monitoring mine equipment in China, helping robots navigate and pick up objects at Willow Garage, detection of swimming pool drowning accidents in Europe, running interactive art in Spain and New York, checking runways for debris in Turkey, inspecting labels on products in factories around the world on to rapid face detection in Japan. It has C++, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS. OpenCV leans mostly towards real-time vision applications and takes advantage of MMX and SSE instructions when available. A full-featured CUDA and OpenCL interfaces are being actively developed right now. There are over 500 algorithms and about 10 times as many functions that compose or support those algorithms. OpenCV is written natively in C++ and has a templated interface that works seamlessly with STL.
OpenCV (Open-Source Computer Vision Library) is released under a BSD license and hence it’s free for both academic and commercial use. It has C++, Python and Java interfaces and supports Windows, Linux, Mac OS, iOS and Android. OpenCV was designed for computational efficiency and with a strong focus on real-time applications. Written in optimized C/C++, the library can take advantage of multi-core processing. Enabled with OpenCL, it can take advantage of the hardware acceleration of the underlying heterogeneous compute platform. Adopted all around the world, OpenCV has more than 47 thousand people of user community and estimated number of downloads exceeding 14 million. Usage ranges from interactive art, to mines inspection, stitching maps on the web or through advanced robotics.


    PYCHARM (IDE)
 PyCharm is an integrated development environment (IDE) used in computer programming, specifically for the Python language. It is developed by the Czech company Jet Brains. It provides code analysis, a graphical debugger, an integrated unit tester, integration with version control systems (VCSes), and supports web development with Django as well as Data Science with Anaconda. PyCharm is cross-platform, with Windows, macOS and Linux versions. The Community Edition is released under the Apache License, and there is also Professional Edition with extra features – released under a proprietary license. PyCharm is a Python IDE with complete set of tools for Python development. In addition, the IDE provides capabilities for professional Web development using the Django framework. Code faster and with more easily in a smart and configurable editor with code completion, snippets, code folding and split windows support. 

PyCharm Features
●	Project Code Navigation - Instantly navigate from one file to another, from method to its declaration or usages, and through classes hierarchy. Learn keyboard shortcuts to be even more productive 
●	Code Analysis - Take advantage of on-the-fly code syntax, error highlighting, intelligent inspections and one-click quick-fix suggestions to make code better.
●	Python Refactoring - Make project-wide code changes painlessly with rename, extract method/superclass, introduce field/variable/constant, move and pull up/push down refactoring’s.
●	Web Development with Django - Even more rapid Web development with Django framework backed up with excellent HTML, CSS and JavaScript editors. Also, with Coffee Script, Mako and Jinja2 support.
●	Google App Engine Support -Develop applications for Google App Engine and delegate routine deployment tasks to the IDE. Choose between Python 2.5 or 2.7 runtime.
●	Version Control Integration - Check in, check-out, view diffs, merge all in the unified VCS user interface for Mercurial, Subversion, Git, Perforce and other SCMs 
●	Graphical Debugger - Fine tune Python or Django applications and unit tests using a full-featured debugger with breakpoints, stepping, frame’s view, watches and evaluate expressions.
●	Integrated Unit Testing - Run a test file, a single test class, a method, or all tests in a folder. Observe results in graphical test runner with execution statistics.
●	Customizable & Extensible - Bundled Text mate, NetBeans, Eclipse &Emacs keyboard schemes, and Vi/Vim emulation plugin.

ALGORITHM:
HAAR classifier
 Haar Cascade is a machine learning object detection algorithm used to identify objects in an image or video and based on the concept of features proposed by Paul Viola and Michael Jones in their paper "Rapid Object Detection using a Boosted Cascade of Simple Features" in 2001.It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images. 
The algorithm has four stages: 
               1. HAAR Feature Selection 
               2. Creating Integral Images 
               3. Adaboost Training 
               4. Cascading Classifiers 

 It is well known for being able to detect faces and body parts in an image, but can be trained to identify almost any object. Let’s take face detection as an example. Initially, the algorithm needs a lot of positive images of faces and negative images without faces to train the classifier. Then we need to extract features from it. First step is to collect the HAAR Features. A HAAR feature considers adjacent rectangular regions at a specific location in a detection window, sums up the pixel intensities in each region and calculates the difference between these sums.

 


 Integral Images are used to make this super-fast. But among all these features we calculated, most of them are irrelevant. For example, consider the image below. Top row shows two good features. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks. The second feature selected relies on the property that the eyes are darker than the bridge of the nose. But the same windows applying on cheeks or any other place is irrelevant. 
 This is accomplished using a concept called Adaboost which both selects the best features and trains the classifiers that use them. This algorithm constructs a “strong” classifier as a linear combination of weighted simple “weak” classifiers. The process is as follows. 
 During the detection phase, a window of the target size is moved over the input image, and for each subsection of the image and Haar features are calculated. You can see this in action in the video below. This difference is then compared to a learned threshold that separates non-objects from objects. Because each Haar feature is only a "weak classifier" (its detection quality is slightly better than random guessing) a large number of Haar features are necessary to describe an object with sufficient accuracy and are therefore organized into cascade classifiers to form a strong classifier. 



 
The cascade classifier consists of a collection of stages, where each stage is an ensemble of weak learners. The weak learners are simple classifiers called decision stumps. Each stage is trained using a technique called boosting. Boosting provides the ability to train a highly accurate classifier by taking a weighted average of the decisions made by the weak learners. Each stage of the classifier labels the region defined by the current location of the sliding window as either positive or negative. Positive indicates that an object was found and negative indicates no objects were found. If the label is negative, the classification of this region is complete, and the detector slides the window to the next location. If the label is positive, the classifier passes the region to the next stage. The detector reports an object found at the current window location when the final stage classifies the region as positive. The stages are designed to reject negative samples as fast as possible. The assumption is that the vast majority of windows do not contain the object of interest. Machine Learning is a rapidly growing Haar Classifier is a supervised classifier, it has mainly been used for facial detection but it can also be trained to detect other objects. Computer vision is such a growing field that there are so many resources available for your own personal use. OpenCV provides a lot of functionality for machine learning techniques and the Haar Classifier is one of them. The Haar Feature supported by OpenCV is an object detector initially proposed by Paul Viola and Michael Jones in Rapid Object Detection using a Boosted Cascade of Simple Features. Example rectangle features shown relative to the enclosing detection window. The sum of the pixels which lie within the white rectangles are subtracted from the sum of pixels in the black rectangles. Two-rectangle features are shown in (A) and (B). Figure (C) shows a three-rectangle feature, and (D) a four-rectangle feature. This technique harbors on the concept of using features rather than pixels directly. Which can then be easier to process and gain domain knowledge from correlating feature sets. It has also proven to be exceptionally faster.




 
The two features are shown in the top row and then overlayed on a typical training cavity in the bottom row. The first feature measures the difference in intensity between the region of the top portion with the region right below. The feature capitalizes on the observation that the top region is often darker than lower parts of the cavity (a gradient). The second feature compares the intensities in the middle regions to the intensity across the outer edges of the cavity.


 
 A Haar Classifier is really a cascade of boosted classifiers working with haar-like features. Haar-like features are specific adjacent rectangular regions at a specific location in a window (as shown in the first image above). The difference is then used to categorize subsections of an image and separates the non-objects from objects. Due to this distinction, it is more of a weak learner, where you must use large number positives to get as many Haar like features as possible to accurately describe an object with some type of correlation and accuracy. So, with all of these essentially weak classifiers we combine them into our classifier cascade to hopefully form a strong learner, by way of boosting. 
Boosting is the concept of creating a set of weak learners and combining them to make a single strong learner. In the implementation of detecting solar cavities, I used the AdaBoosting algorithm (aka Adaptive Boosting). This algorithm was initially proposed by Yoav Freund and Robert Schapire in A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting. It is used to improve a learning algorithms performance, by calling weak classifiers repeatedly and updating weights. OpenCV currently supports multiple variations of AdaBoost including Discrete Adaboost, Real Adaboost, Gentle Adaboost and Logit boost.
4.1. System Architecture
 
FIG 4.1: SYSTEM ARCHITECTURE


4.2. Data Flow Diagram 
DFD is the abbreviation for Data Flow Diagram. The flow of data of a system or a process is represented by DFD. It also gives insight into the inputs and outputs of each entity and the process itself. DFD does not have control flow and no loops or decision rules are present. Specific operations depending on the type of data can be explained by a flowchart. Data Flow Diagram can be represented in several ways. 
 



 
Fig 4.2: DATAFLOW DIAGRAM
4.3. UML DIAGRAMS
The UML stands for Unified modelling language, is a standardized general-purpose visual modelling language in the field of Software Engineering. It is used for specifying, visualizing, constructing, and documenting the primary artifacts of the software system. It helps in designing and characterizing, especially those software systems that incorporate the concept of Object orientation. It describes the working of both the software and hardware systems. The UML was developed in 1994-95 by Grady Brooch, Ivar Jacobson, and James Rumbaugh at the Rational Software. In 1997, it got adopted as a standard by the Object Management Group (OMG). The Object Management Group (OMG) is an association of several companies that controls the open standard UML. The OMG was established to build an open standard that mainly supports the interoperability of object-oriented systems. It is not restricted within the boundaries, but it can also be utilized for modelling the non-software systems. 
Goals Of UML
o	Since it is a general-purpose modelling language, it can be utilized by all the modelers.
o	UML came into existence after the introduction of object-oriented concepts to systemize and consolidate the object-oriented development, due to the absence of standard methods at that time.
o	The UML diagrams are made for business users, developers, ordinary people, or anyone who is looking forward to understand the system, such that the system can be software or non-software.

4.3.1. Use Case Diagram:
A use case diagram in the Unified Modeling Language (UML) is a type of behavioral diagram defined by and created from a Use-case analysis. Its purpose is to present a graphical overview of the functionality provided by a system in terms of actors, their goals (represented as use cases), and any dependencies between those use cases. The main purpose of a use case diagram is to show what system functions are performed for which actor. Roles of the actors in the system can be depicted.
 
Fig 4.3.1: USE CASE Diagram

4.3.2. Class Diagram:
The class diagram depicts a static view of an application. It represents the types of objects residing in the system and the relationships between them. A class consists of its objects, and also it may inherit from other classes. A class diagram is used to visualize, describe, document various different aspects of the system, and also construct executable software code. It shows the attributes, classes, functions, and relationships to give an overview of the software system. It constitutes class names, attributes, and functions in a separate compartment that helps in software development. Since it is a collection of classes, interfaces, associations, collaborations, and constraints, it is termed as a structural diagram.
It is the only diagram that is widely used for construction, and it can be mapped with object-oriented languages. It is one of the most popular UML diagrams. Following is the purpose of class diagrams given below:
1.	It analyses and designs a static view of an application.
2.	It describes the major responsibilities of a system.
3.	It is a base for component and deployment diagrams.
4.	It incorporates forward and reverse engineering.
 
Fig 4.3.2: CLASS Diagram



4.3.3. Sequence Diagram
The sequence diagram represents the flow of messages in the system and is also termed as an event diagram. It helps in envisioning several dynamic scenarios. It portrays the communication between any two lifelines as a time-ordered sequence of events, such that these lifelines took part at the run time. In UML, the lifeline is represented by a vertical bar, whereas the message flow is represented by a vertical dotted line that extends across the bottom of the page. It incorporates the iterations as well as branching.
1.	To model high-level interaction among active objects within a system.
2.	To model interaction among objects inside a collaboration realizing a use case.
3.	It either model’s generic interactions or some certain instances of interaction.
 
Fig 4.3.3: SEQUENCE Diagram









CHAPTER 5
IMPLEMENTATION
Implementation is the stage of the project when the theoretical design is turned out into a working system. Thus, it can be considered to be the most critical stage in achieving a successful new system and in giving the user confidence that the new system will work and be effective. 
The implementation stage involves careful planning, investigation of the existing system and its constraints on implementation, designing of methods to achieve changeover and evaluation of changeover methods
PURPOSE
A system with any operating system must have python (>= 3.3), OpenCV (python3- version) and           PyCharm installed as Functional Requirement. As a part of Non-functional requirements, or performance requirements or quality of service requirements, the system must be usable, available, reliable, supportable, testable and maintainable.

SCOPE 
The time taken to generate the result is very less. The accuracy of the project may be increased with the by providing the dataset. At present, the project is detecting emotion further analysis can be added for increased performance. Robust spontaneous emotion recognizer can be developed and deployed in real-time systems. This is going to have an impact in Security surveillance for detecting the emotion.

OVERALL DESCRIPTION 
Emotion detection is a crucial and challenging problem in visual surveillance. Automatic monitoring of the emotion is important for safety control and urban planning. For this purpose, many techniques and methods have been proposed. These techniques are not producing high performance and better accuracy for complicated scenes. Recently Foreground Extraction and Background subtraction-based methods are proposed, which provides a better accurate solution for emotion detection. This literature survey discusses some of the existing methods and their performance. Emotion detection is an end user application which detects the motion from the real time capturing being given to the system. We have implemented HAAR classifier for motion detection.

    ECONOMIC FEASIBILITY
The system is economically feasible. It does not require any addition hardware or software besides the normal working system with Python (>= 3.3), OpenCV (python3- version), PyCharm GUI. Since the interface for this system is developed using the existing resources and technologies available online. There is no expenditure for certain.

5.1 SYSTEM REQUIREMENT SPECIFICATION
5.1.1 SOFTWARE REQUIREMENTS
These software requirements are the specification of the system. The software requirements provide a basis for creating the software requirements specification. These are useful in estimating cost, planning team activities, performing tasks and tracking the teams and tracking the team’s programs throughout the development activity.
●	 Operating System: Windows XP 
●	 Coding Language: Python
●	 Front End: PyCharm GUI

5.1.2 HARDWARE REQUIREMENTS 
These hardware requirements serve as the basis for a contract for the implementation of the system and hence are a complete and consistent specification of the whole system. These may be used by software engineers as the starting point for the system design. These specify what the system does and not how it should be implemented. 
●	Processor - Pentium –IV 
●	Speed - 1.1 GHz
●	Ram - 256 Mb 
●	Hard Disk - 20 Gb
●	Key Board - Standard Windows Keyboard 
●	Mouse - Two or Three Button Mouse
●	Any type of camera

5.2 SYSTEM ENVIRONMENT
5.2.1 PYTHON
Python is a high-level, interpreted, interactive and object-oriented scripting language. Python is designed to be highly readable. It uses English keywords frequently where as other languages use punctuation, and it has fewer syntactical constructions than other languages.
•	Python is Interpreted: Python is processed at runtime by the interpreter. You do not need to compile your program before executing it. This is similar to PERL and PHP.
•	Python is Interactive: You can actually sit at a Python prompt and interact with the interpreter directly to write your programs.
•	Python is Object-Oriented: Python supports Object-Oriented style or technique of programming that   encapsulates code within objects.
•	Python is a Beginner's Language: Python is a great language for the beginner-level programmers and       supports the development of a wide range of applications from simple text processing to WWW browsers to games.
5.2.2 History of Python
Python was developed by Guido van Rossum in the late eighties and early nineties at the National Research Institute for Mathematics and Computer Science in the Netherlands.
Python is derived from many other languages, including ABC, Modula-3, C, C++, Algol-68, Smalltalk, and Unix shell and other scripting languages.
Python is copyrighted. Like Perl, Python source code is now available under the GNU General Public License (GPL).
Python is now maintained by a core development team at the institute, although Guido van Rossum still holds a vital role in directing its progress.

5.2.3 Python Features
          Python's features include:
•	Easy-to-learn: Python has few keywords, simple structure, and a clearly defined syntax. This allows the student to pick up the language quickly.
•	Easy-to-read: Python code is more clearly defined and visible to the eyes.
•	Easy-to-maintain: Python's source code is fairly easy-to-maintain.
•	A broad standard library: Python's bulk of the library is very portable and cross-platform compatible on UNIX, Windows, and Macintosh.
•	Interactive Mode: Python has support for an interactive mode which allows interactive testing and debugging of snippets of code.
•	Portable: Python can run on a wide variety of hardware platforms and has the same interface on all platforms.
•	Extendable: You can add low-level modules to the Python interpreter. These modules enable programmers to add to or customize their tools to be more efficient.
•	Databases: Python provides interfaces to all major commercial databases.
•	GUI Programming: Python supports GUI applications that can be created and ported to many system calls, libraries and windows systems, such as Windows MFC, Macintosh, and the X Window system of Unix.
•	Scalable: Python provides a better structure and support for large programs than shell scripting.
Python has a big list of good features:
•	It supports functional and structured programming methods as well as OOP.
•	It can be used as a scripting language or can be compiled to byte-code for building large applications.
•	It provides very high-level dynamic data types and supports dynamic type checking.
•	IT supports automatic garbage collection.
It can be easily integrated with C, C++, COM, ActiveX, CORBA, and Java
•	.
5.2.4. PYTHON LIBRARIES Python's syntax, semantics, and tokens are all contained in the Python Standard Library. It comes with built-in modules that give the user access to basic functions like I/O and a few other essential modules. The Python libraries have been written in the C language for the most part. There are over 200 core modules in the Python standard library. Python is a powerful programming language because of all of these factors. The Python Standard Library is extremely important. Programmers won't be able to use Python's features unless they have it. Apart from that, Python has several libraries that make a programmer's life easier. Let us study some of the most popular libraries:
    Matplotlib
The plotting of numerical data is the responsibility of this library. It's for this reason that it's used in analysis of data. It's an open-source library that plots high-definition figures such as pie charts, scatterplots, boxplots, and graphs, among other things.
NumPy
NumPy is one of the most widely used open-source Python packages, focusing on mathematical and scientific computation. It has built-in mathematical functions for convenient computation and facilitates large matrices and multidimensional data. It can be used for various things, including linear algebra, as an N-dimensional container for all types of data. The NumPy Array Python object defines an N-dimensional array with rows and columns. A long with this, it can be used as a random number generator.
Pandas
Pandas is an open-source library licenced under the Berkeley Software Distribution (BSD). In the domain of data science, this well-known library is widely used. They're mostly used for analysis, manipulation, and cleaning of data, among other things. Pandas allows us to perform simple data modelling and analysis without having to swap to another language like R.
SciPy
Scipy is a Python library. It is an open-source library, especially designed for scientific computing, information processing, and high-level computing. A large number of user-friendly methods and functions for quick and convenient computation are included in the library. Scipy can be used for mathematical computations alongside NumPy.
Scikit- learn
Scikit-learn is also an open-source machine learning library based on Python. Both supervised and unsupervised learning processes can be used in this library. Popular algorithms and the SciPy, NumPy, and Matplotlib packages are all already pre-included in this library. The most well-known Scikit-most-learn application is for Spotify music recommendations.
Seaborn
Visualization of statistical models is possible with this package. The library is largely based on Matplotlib and enables the formation of statistical graphics via:
•	Variable comparison via an API based on datasets
•	Create complex visualisations with ease, including multi-plot grids.
•	Univariate and bivariate visualisations are used to compare data subsets.
•	Patterns can be displayed in a variety of colour palettes.
•	Linear regression estimation and plotting are done automatically.
TensorFlow
TensorFlow is an open-source numerical calculation library with high performance. Deep learning and ML algorithms make use of it as well. It was developed by Google Brain group researchers inside the Google AI organisation and is now widely used for complex mathematical computations by mathematics, physics, and also machine learning researchers.
Keras
Keras is a Python-based open-source neural network library that makes it possible for us to examine deep neural networks deeply. As deep learning becomes more common, Keras emerges as a viable option because, according to its creators, it is an API (Application Programming Interface) designed for humans, not machines. Compared to TensorFlow or Theano, Keras has a greater adoption rate in the research community and industry. Before installing Keras, the user should first download the TensorFlow backend engine.
        Scrapy
Scrapy is a web scraping tool that scrapes multiple pages in under a minute. Scrapy is also an open-source Python library framework for extracting data from websites. Under the name "Scraping hub ltd," it is a high-speed, high-level scraping and crawling web library
PyGame
This library provides a simple interface for the graphics, audio, and input libraries of the Standard Direct media Library (SDL) which can work on any platform. It's used to make video games with the Python programming language and computer graphics and acoustic libraries.
PyBrain
PyBrain is a fast and simple machine learning library compared to the other Python learning libraries. PyBrain is also an open-source library for ML algorithms for every entry-level scholar in research from the available Python libraries. PyBrain's main aim is to provide ML algorithms that are both flexible and convenient to use by even entry-level coders. It also comes with pre-built environments for comparing algorithms.
5.2.5. MACHINE LEARNING
Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.
Recommendation engines are a common use case for machine learning. Other popular uses include fraud detection, spam filtering, malware threat detection, business process automation (BPA) and Predictive maintenance.
TYPES OF MACHINE LEARNING
Classical machine learning is often categorized by how an algorithm learns to become more accurate in its predictions. There are four basic approaches: 
•	Supervised learning: In this type of machine learning, data scientists supply algorithms with labelled training data and define the variables they want the algorithm to assess for correlations. Both the input and the output of the algorithm is specified.
•	Unsupervised learning: This type of machine learning involves algorithms that train on unlabelled data. The algorithm scans through data sets looking for any meaningful connection. The data that algorithms train on as well as the predictions or recommendations they output are predetermined.
•	Semi-supervised learning: This approach to machine learning involves a mix of the two preceding types. Data scientists may feed an algorithm mostly labelled training data, but the model is free to explore the data on its own and develop its own understanding of the data set.
•	Reinforcement learning: Data scientists typically use reinforcement learning to teach a machine to complete a multi-step process for which there are clearly defined rules. Data scientists program an algorithm to complete a task and give it positive or negative cues as it works out how to complete a task. But for the most part, the algorithm decides on its own what steps to take along the way.
5.3 SOURCE CODE:
•	import cv2
•	from deep face import Deep Face
•	import NumPy as np
•	
•	face cascade = cv2.CascadeClassifier(cv2.data. haarcascades + 'haarcascade_frontalface_default.xml')
•	
•	video = cv2.VideoCapture(0)
•	while video.isOpened():
•	    _,frame = video.read()
•	    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
•	    face=face_cascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5)
•	
•	    for x,y,w,h in face:
•	        img = cv2.rectangle(frame,(x,y),(x+w, y+h), (0,255,0), 2)
•	        try:
•	            analyze = DeepFace.analyze(frame,actions=['emotion'])
•	            print(analyze['dominant_emotion'])
•	        except:
•	            print("no face")
•	
•	
•	    cv2.imshow('video',frame)
•	    key=cv2.waitKey(5)
•	    if key==ord('q'):
•	        break;
•	video. release ()
 
CHAPTER 6
TESTING
6.1. SYSTEM TESTING: 
The purpose of testing is to discover errors. Testing is the process of trying to discover every conceivable fault or weakness in a work product. It provides a way to check the functionality of components, sub-assemblies, assemblies and/or a finished product. It is the process of exercising software with the intent of ensuring that the software system meets its requirements and user expectations and does not fail in an unacceptable manner. There are various types of tests. Each test type addresses a specific testing requirement.
 6.2. TYPES OF TESTS: 
6.2.1 UNIT TESTING: 
Unit testing involves the design of test cases that validate that the internal program logic is functioning properly, and that program inputs produce valid outputs. All decision branches and internal code flow should be validated. It is the testing of individual software units of the application. It is done after the completion of an individual unit before integration. This is a structural testing that relies on knowledge of its construction and is invasive. Unit tests perform basic tests at component level and test a specific business process, application, and/or system configuration. Unit tests ensure that each unique path of a business process performs accurately to the documented specifications and contains clearly defined inputs and expected results. 
 6.2.2 INTEGRATION TESTING: 
Integration tests are designed to test integrated software components to determine if they actually run as one program. Testing is event driven and is more concerned with the basic outcome of screens or fields. Integration tests demonstrate that although the components were individually satisfied, as shown by successfully unit testing, the combination of components is correct and consistent. Integration testing is specifically aimed at exposing the problems that arise from the combination of components. 
 6.2.3 FUNCTIONAL TEST:
 Functional tests provide systematic demonstrations that functions tested are available as specified by the business and technical requirements, system documentation, and user manuals. 
Functional testing is cantered on the following items:
● Valid Input -        identified classes of valid input must be accepted.
● Invalid Input -      identified classes of invalid input must be rejected.
● Functions -           identified functions must be exercised. 
● Output -                identified classes of application outputs to be tested. 
● Systems/Procedures -   interfacing systems or procedures must be invoked. 	
Organization and preparation of functional tests is focused on requirements, key functions, or special test cases. In addition, systematic coverage pertaining to identify Business process flows; data fields, predefined processes, and successive processes must be considered for testing. Before functional testing is complete, additional tests are identified and the effective value of current tests is determined. 
6.2.4 SYSTEM TESTING:
System testing ensures that the entire integrated software system meets requirements. It tests a configuration to ensure known and predictable results. An example of system testing is the configuration-oriented system integration test. System testing is based on process descriptions and flows, emphasizing pre-driven process links and integration points.
6.2.5 WHITE BOX TESTING: 
White Box Testing is a testing in which the software tester has knowledge of the inner workings, structure and language of the software, or at least its purpose. It is purposeful. It is used to test areas that cannot be reached from a black box level. 
6.2.6 BLACK BOX TESTING: 
Black Box Testing is testing the software without any knowledge of the inner workings, structure or language of the module being tested. Black box tests, as most other kinds of tests, must be written from a definitive source document, such as specification or requirements document, such as specification or requirements document.
 It is a test in which the software under test is treated as a black box. You cannot “see” into it. The test provides inputs and responds to outputs without considering how the software works. 
6.3. UNIT TESTING: 
In unit testing different are modules are tested against the specifications produced during the design for the modules. Unit testing is essential for verification of the code produced during the coding phase, and hence the goals to test the internal logic of the modules. Using the detailed design description as a guide, important Conrail paths are tested to uncover errors within the boundary of the modules. This testing is carried out during the programming stage itself. In this type of testing step, each module was found to be working satisfactorily as regards to the expected output from the module.  

        
TEST STRATEGY AND APPROACH:
 A strategy for system testing integrates system test cases and design techniques into a well-planned series of steps that results in the successful construction of software. The testing strategy must co-operate test planning, test case design, test execution, and the resultant data collection and evaluation. A strategy for software testing must accommodate low-level tests that are necessary to verify that a small source code segment has been correctly    implemented   as well as high level tests that validate   major system functions against user requirements.
TEST OBJECTIVES: 
● All field entries must work properly. 
● Pages must be activated from the identified link. 
● The entry screen, messages and responses must not be delayed. 
FEATURES TO BE TESTED:
● Verify that the entries are of the correct format 
● No duplicate entries should be allowed 
● All links should take the user to the correct page. 
6.4 INTEGRATION TESTING: 
Software integration testing is the incremental integration testing of two or more integrated software components on a single platform to produce failures caused by interface defects. The task of the integration test is to check that components or software applications, e.g., components in a software system or – one step up – software applications at the company level – interact without error. Test Results: All the test cases mentioned above passed successfully. No defects encountered. 
The following are the types of Integration Testing:
1.Top-Down Integration  
This method is an incremental approach to the construction of program structure.  Modules are integrated by moving downward through the control hierarchy, beginning with the main program module. The module subordinates to the main program module are incorporated into the structure in either a depth first or breadth first manner.
2.  Bottom-up Integration 
This method begins the construction and testing with the modules at the lowest level in the program structure. Since the modules are integrated from the bottom up, processing required for modules subordinate to a given level is always available and the need for stubs is eliminated. 


 7.5 ACCEPTANCE TESTING: 
User Acceptance Testing is a critical phase of any project and requires significant participation by the   end user. It also ensures that the system meets the functional requirements. 
Test Results: All the test cases mentioned above passed successfully. No defects encountered.




































CHAPTER 7
SCREEN SHOTS




 













CHAPTER 8
CONCLUSION AND FUTURE SCOPE
9.1
8.1 CONCLUSION:
An emotion detection system was thus developed successfully in this paper. This system mainly provides an efficient method for detecting emotions and is aimed to be highly beneficial for any person or organization. Thus, an emotion-based change detection in a video format was completed and successfully implemented. The future scope of the work done could be as follows: the due course of time as we started to understand the minute details of our work, we significantly realized that our software would be tremendously important in the future world. Following changes or additions can be done on our work to include some new features. 
• With the existing system, advancement can be included and relevant songs can be played to the user when specific emotion is detected. 
• The stored video can be automatically transferred to some email account so that an extra backup data can be used. 
• A facility for the user can be given where he can mainly monitor only a small specific area in the range of the web cam. 
• In the future, the user can be provided a remote access to this software from some remote PC through internet. 
• Include an option to take snaps periodically, manually or automatically. 
• Work could be done to make the system more users friendly for a layman user.




